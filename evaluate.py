import logging
from openai import OpenAI
from utils import file_to_string, code_from_response, block_print, enable_print

# This loads the task dependent system prompt
SYSTEM_PROMPT = file_to_string("system_prompt.txt")

def gen_sample(context: str, model_name = "gpt-4", temperature = 0.7) -> str:
    """
    Uses the OpenAI API for the LLM given by model_name to generate a sample to the task, 
    assisted by the context which describes the task as well as the reflection
    for the most successful sample of the previous generation.
    """
    client = OpenAI()
    
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": context}
                ],
        temperature=temperature
            )
    
    sample_raw = response.choices[0].message.content

    return code_from_response(sample_raw)

# The rule here is to remove the first appearence of each value
X = [[97, 97, 97, 97],[4, 4, 4],[33, 0, 4, 1, 2, 24, 66],[76, 42, 17, 76, 17],[12,9,12]]
Y = [[97, 97, 97],[4, 4],[],[76, 17],[12]]

assert(len(X) == len(Y))

def run_generated_code(sample_code, x):
    """
    This function runs the LLM generated code using exec(). Exec is dangerous! Run at your own risk.
    """
    try:
        block_print()
        exec(sample_code+f"\nprediction = F({x})", globals())
        enable_print()

        # This variable is set by the exec()
        return prediction

    except Exception as e:
        # This might be fine! LLM generated code often fails to compile.
        logging.debug(f"Exception when running LLM generated code: {e}")
        logging.debug(f"The code that failed to run is:\n\n{sample_code}\n")

    return []

def evaluate(sample: str) -> float:
    """
    Given a sample generated by the LLM, evaluate it on the task, returning the loss that the evolGPT wants to minimize.
    """

    loss = 0

    # In the example, the loss is the number of failed examples.
    for i in range(len(X)):
        prediction = run_generated_code(sample, X[i])

        if prediction != Y[i]:
            loss += 1

    return loss

def reflect(sample: str) -> str:
    """
    Given a sample generated by the LLM, provide feedback on its performance on the task, which would be helpful to improve it.
    """

    reflection_text = f"\n\nYour solution attempt: \n\n {sample}\n\nDoesn't work for the following examples:\n"

    

    # In the example, the reflection is the examples and the y predicted for them by the current candidate sample.
    for i in range(len(X)):
        # Running exec is dangerous! Do this at your own risk!
        prediction = run_generated_code(sample, X[i])
        if prediction != Y[i]:
            reflection_text += f"\nShould be {X[i]} - > {Y[i]}, but currently predicts {prediction}."
    
    return reflection_text

def get_initial_context():
    """
    The context always starts with all of the examples X -> Y, the reflections will be appended to it.
    """
    examples = ""

    for i in range(len(X)):
        examples += f"{X[i]} - > {Y[i]}\n"

    return examples